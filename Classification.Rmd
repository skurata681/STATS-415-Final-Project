---
title: "Classification"
author: "Sarah Kurata"
date: "3/27/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(readr)
library(dplyr)
library(tidyverse)

#libraries for subset methods
library(SignifReg)
library(leaps)

#shrinkage methods
library(glmnet)
library(pls)
library(lares)
#LDA and QDA model
library(MASS)

#Tree library
library(tree)
#RandomForest
library(randomForest)
#gradient boosting
library(gbm)

#support vector classifier
library(e1071)

#KNN library
library(FNN)
```

## Clean data and split into training and test subsets

```{r}
df = read.csv('./Data/full_data.csv')
```


## Testing "on special diet?" as response (DRQSDIET)

### Data cleaning

```{r Classification Test and Training Data}
#CREATING THE TEST AND TRAINING DATA

#first take out NA's in data frame we will use
data_diet <-drop_na(data, DRQSDIET, BMIWT, LBXTC, BPXSY1, DR1_330Z, DR1TCAFF)

#first split in test and train for whole data frame
train_idx <- sample(1:nrow(data_diet), size = floor(.7 * nrow(data_diet)))

#test and training data
train_data <- data_diet[train_idx, ]
test_data <- data_diet[-train_idx, ]

#change to factor for test data
test_data<-filter(test_data, DRQSDIET != 9)
test_data$DRQSDIET01 <- ifelse(test_data$DRQSDIET == 2, 1, 0)

#change to factor for training data
train_data<-filter(train_data, DRQSDIET != 9)
train_data$DRQSDIET01 <- ifelse(train_data$DRQSDIET == 2, 1, 0)
```


```{r KNN}
#SCALE FOR KNN
#calculate mean and sd
mean_train <- colMeans(train_data[c('BMIWT','LBXTC', 'BPXSY1', 'DR1_330Z', 'DR1TCAFF')])
sd_train <- sqrt(diag(var(train_data[c('BMIWT','LBXTC', 'BPXSY1', 'DR1_330Z', 'DR1TCAFF')])))
#scale the training and test data
X_train_scaled <- scale(train_data[c('BMIWT','LBXTC', 'BPXSY1', 'DR1_330Z', 'DR1TCAFF')], center = mean_train, scale = sd_train)
X_test_scaled <- scale(test_data[c('BMIWT','LBXTC', 'BPXSY1', 'DR1_330Z', 'DR1TCAFF')], center = mean_train, scale = sd_train)


# Cross validation to find optimal K
Ks <- seq(1, 30, by = 1)

#vectors to store the MSEs
trainMSEs <- c()
testMSEs <- c() 

for(K in Ks) {
knnTrain <- knn(train = X_train_scaled, cl = train_data$DRQSDIET,
test = X_train_scaled, k = K)
knnTest <- knn(train = X_train_scaled, cl = train_data$DRQSDIET,
test = X_test_scaled, k = K)

trainMSE <- mean(knnTrain != train_data$DRQSDIET)
testMSE <- mean(knnTest != test_data$DRQSDIET)

trainMSEs <- c(trainMSEs, trainMSE)
testMSEs <- c(testMSEs, testMSE)
}
#plot the train and test errors against the K value
plot(Ks, trainMSEs, type = "b", lwd = 2, col = "blue", xlab = "K", ylab = "MSE", ylim = c(0,.4))
lines(Ks, testMSEs, type = "b", lwd = 2, col = "red")
legend("bottomright", legend = c("Train MSE", "Test MSE"), col = c("blue", "red"), lwd = c(2,2))

#which K produces the minimum for testMSEs
x = Ks[which.min(testMSEs)]

#KNN with this ideal K
KNN_11<-knn(train = X_train_scaled, cl = train_data$DRQSDIET,
test = X_test_scaled, k = x)
#test error
mean(KNN_11 != test_data$DRQSDIET)
#confusion matrix
table(KNN_11, test_data$DRQSDIET)
```


```{r AdaBoost}

#Adaboost
#make adaboost model (boosted tree)
adaboost_mod <- gbm(
DRQSDIET01~ BMIWT + LBXTC + BPXSY1 + DR1_330Z + DR1TCAFF, 
data=train_data,
distribution="adaboost",
n.trees=1000)

#variable importance plot
summary(adaboost_mod)

#find test error
adaboost_test_probs <- predict(adaboost_mod, test_data, n.trees=1000, type='response')
adaboost_test_preds <- ifelse(adaboost_test_probs > 0.5, 1, 0)
test_err <- mean(adaboost_test_preds != test_data$DRQSDIET01)
print(test_err)
```


```{r Support Vector Classifier}

```

