---
title: "STATS 415 Final Project"
author: "Sarah Kurata, Hannah Daane, Saman Verma"
date: "3/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#for repreduciability
set.seed(245)

library(haven)
library(readr)
library(dplyr)
library(tidyverse)

#libraries for subset methods
library(SignifReg)
library(leaps)
#shrinkage methods
library(glmnet)
library(pls)
library(lares)
#LDA and QDA model
library(MASS)

#Tree library
library(tree)
#RandomForest
library(randomForest)
#gradient boosting
library(gbm)

#support vector classifier
library(e1071)

#neural network
library(keras)
library(neuralnet)
library(superml)
library(tfruns)

#KNN library
library(FNN)
```

## Reading in Data

```{r, echo=FALSE}
#2009
bmx09 <- read_xpt("Data/2009/BMX_F.XPT")
bpx09 <- read_xpt("Data/2009/BPX_F.XPT")
demo09 <- read_xpt("Data/2009/DEMO_F.XPT")
dr109 <- read_xpt("Data/2009/DR1TOT_F.XPT")
smq09 <- read_xpt("Data/2009/SMQ_F.XPT")
tchol09 <- read_xpt("Data/2009/TCHOL_F.XPT")

data09 = merge(bmx09, bpx09, by = "SEQN", all.x = TRUE)
listy = list(demo09, dr109, smq09, tchol09)
for(i in 1:4){
  data09 = merge(data09, listy[[i]], by = "SEQN", all.x = TRUE)
}
write_csv(data09, "Data/2009/data2009.csv")

#2011
bmx11 <- read_xpt("Data/2011/BMX_G.XPT")
bpx11 <- read_xpt("Data/2011/BPX_G.XPT")
demo11 <- read_xpt("Data/2011/DEMO_G.XPT")
dr111 <- read_xpt("Data/2011/DR1TOT_G.XPT")
smq11 <- read_xpt("Data/2011/SMQ_G.XPT")
tchol11 <- read_xpt("Data/2011/TCHOL_G.XPT")

data11 = merge(bmx11, bpx11, by = "SEQN", all.x = TRUE)
listy = list(demo11, dr111, smq11, tchol11)
for(i in 1:4){
  data11 = merge(data11, listy[[i]], by = "SEQN", all.x = TRUE)
}
write_csv(data11, "Data/2011/data2011.csv")

#2013
bmx13 <- read_xpt("Data/2013/BMX_H.XPT")
bpx13 <- read_xpt("Data/2013/BPX_H.XPT")
demo13 <- read_xpt("Data/2013/DEMO_H.XPT")
dr113 <- read_xpt("Data/2013/DR1TOT_H.XPT")
smq13 <- read_xpt("Data/2013/SMQ_H.XPT")
tchol13 <- read_xpt("Data/2013/TCHOL_H.XPT")

data13 = merge(bmx13, bpx13, by = "SEQN", all.x = TRUE)
listy = list(demo13, dr113, smq13, tchol13)
for(i in 1:4){
  data13 = merge(data13, listy[[i]], by = "SEQN", all.x = TRUE)
}
write_csv(data13, "Data/2013/data2013.csv")

#2015
bmx15 <- read_xpt("Data/2015/BMX_I.XPT")
bpx15 <- read_xpt("Data/2015/BPX_I.XPT")
demo15 <- read_xpt("Data/2015/DEMO_I.XPT")
dr115 <- read_xpt("Data/2015/DR1TOT_I.XPT")
smq15 <- read_xpt("Data/2015/SMQ_I.XPT")
tchol15 <- read_xpt("Data/2015/TCHOL_I.XPT")

data15 = merge(bmx15, bpx15, by = "SEQN", all.x = TRUE)
listy = list(demo15, dr115, smq15, tchol15)
for(i in 1:4){
  data15 = merge(data15, listy[[i]], by = "SEQN", all.x = TRUE)
}
write_csv(data15, "Data/2015/data2015.csv")

#2017
bmx17 <- read_xpt("Data/2017/BMX_J.XPT")
bpx17 <- read_xpt("Data/2017/BPX_J.XPT")
demo17 <- read_xpt("Data/2017/DEMO_J.XPT")
dr117 <- read_xpt("Data/2017/DR1TOT_J.XPT")
smq17 <- read_xpt("Data/2017/SMQ_J.XPT")
tchol17 <- read_xpt("Data/2017/TCHOL_J.XPT")

data17 = merge(bmx17, bpx17, by = "SEQN", all.x = TRUE)
listy = list(demo17, dr117, smq17, tchol17)
for(i in 1:4){
  data17 = merge(data17, listy[[i]], by = "SEQN", all.x = TRUE)
}
write_csv(data17, "Data/2017/data2017.csv")

#Total
data = bind_rows(data09,data11)
listy = list(data13,data15,data17)
for(i in 1:3){
  data = bind_rows(data, listy[[i]])
}
write_csv(data,"Data/full_data.csv")

train = read_csv("Data/Kaggle/train.csv")
train = merge(train, data, by = "SEQN", all.x = TRUE)
train=train[, colSums(is.na(train))==0]
train = train[, -which(names(train)%in%c("SMDUPCA", "SMD100BR", "DR1DRSTZ", "DRABF", "RIDSTATR"))]
write_csv(train, "Data/Total/train.csv")
test = read_csv("Data/Kaggle/test.csv")
test = merge(test, data, by = "SEQN", all.x = TRUE)
test=test[, colSums(is.na(test))==0]
test = test[, -which(names(test)%in%c("SMDUPCA", "SMD100BR", "DR1DRSTZ", "DRABF", "RIDSTATR"))]
drops = c(setdiff(names(test), names(train)))
test = test[,!(names(test) %in% drops)]
write_csv(test, "Data/Total/test.csv")
```


```{r}
#original attempt
view(train)
model1 = glm(y ~ BMDSTATS, data = train)
x = predict(model1, newdata = test)
firstprediction = data.frame(test$SEQN, x)
firstprediction <-  firstprediction%>%rename(SEQN=test.SEQN)
firstprediction<-  firstprediction%>%rename(y=x)
write_csv(firstprediction, "Predictions/Kaggle/firstPrediction.csv")


#KAGGLE ATTEMPTS FORWARD SELECTION, this method looks at p-values
nullmodel <- lm(y ~ 1, data = non_correlated)
fullmodel <- lm(y ~ ., data = non_correlated)
select.p.fwd <- SignifReg(fit = nullmodel,
scope = list(lower = formula(nullmodel), upper = formula(fullmodel)),
alpha = 0.05, direction = "forward",
adjust.method = "none", trace = FALSE)
summary(select.p.fwd)

#resulting model from p-value
forward_selection_model_signifreg <-lm(formula = y ~ SMAQUEX2 + BMXARML + DMDBORN4 + BPXSY1 + BPACSZ + DR1TSUGR + DR1TFIBE + RIDAGEYR + DR1TKCAL + BMXWT + BMXBMI + BMXHT + BMXWAIST + RIDEXMON + BMXARMC + SIAPROXY + DR1TATOC + FIALANG + DR1TS100 + SEQN + LBXTC + SIAINTRP + WTINT2YR, data = train)
summary(forward_selection_model_signifreg)

#Random Forest Model  
randomForest(y ~., data = train, mtry = 48, importance = TRUE)

#Neural Network
neuralnetmodel<-neuralnet(y ~ ., train, hidden = 4500)
predict_testNN = compute(neuralnetmodel, test)
cor(predict_testNN$net.result,train$y[1:3823])^2

scale_train<-scale(train)
scale_train<-as.data.frame(scale_train)
test$y <- seq(1:3823)
scale_test<-scale(test)
scale_test<-as.data.frame(scale_test)
# scale_train <- scale_train[sample(1:nrow(scale_train)), ]
#training and testing data for neural networks
test_idx <- sample(seq(nrow(scale_train)), size= round(nrow(scale_train)*.3)) 
train_idx <- seq(nrow(scale_train))[-test_idx]
train_neural_networks <- scale_train[train_idx,]
test_neural_networks <- scale_train[test_idx,]

#new attempt at neural network
xTr = model.matrix(lm(y ~ . - 1, data=train_neural_networks))
yTr = train_neural_networks$y
xTe = model.matrix(lm(y ~ . - 1, data=test_neural_networks))
yTe = test_neural_networks$y

xTrain = model.matrix(lm(y ~ . - 1, data=scale_train))
yTrain = scale_train$y
xTest = model.matrix(lm(y ~ . - 1, data=scale_test))

#NEURAL NETWORK
#####################################################################################################################################
modnn_2 <- keras_model_sequential () %>%
layer_dense(units = 64, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
layer_dropout(rate=0.4) %>%
layer_dense(units = 64, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
layer_dense(units = 62, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
layer_dropout(rate=0.4) %>%
layer_dense(units = 64, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
layer_dropout(rate=0.5) %>%
  layer_dense(units = 1)
modnn_2 %>% compile(loss = "mse",
optimizer = optimizer_adam(learning_rate=0.005))
start_time <- Sys.time()
history <- modnn_2 %>% fit(
xTrain, yTrain, epochs = 300, batch_size = 4500
)
end_time <- Sys.time()
end_time - start_time
#l 1 incentivizes things to go all the way to 0
#l 2 regularization typical (not a big difference)
npred <- c(predict(modnn_2, xTest))
(plain_net <- cor(npred, yTe)^2) # test r^2


neural_network_prediction = data.frame(test$SEQN, npred)

colnames(neural_network_prediction)
neural_network_prediction <-  neural_network_prediction%>%rename(SEQN=test.SEQN)
neural_network_prediction<-  neural_network_prediction%>%rename(y=npred)
write_csv(neural_network_prediction, "Predictions/Kaggle/neural_network_Prediction.csv")

#####################################################################################################################################
#look at different combinations in order to find ideal value of each flag or different parameter
FLAGS <- flags(
  flag_numeric("dropout1", 0.4),
  flag_numeric("dropout2", 0.3),
  flag_numeric("dropout3", 0.4),
  flag_numeric("dropout4", 0.3),
  flag_numeric("units1", 20),
  flag_numeric("units2", 20),
  flag_numeric("units3", 20),
  flag_numeric("units4", 20),
  flag_numeric("regulizar1", 0.1),
  flag_numeric("regulizar2", 0.1),
  flag_numeric("regulizar3", 0.1),
  flag_numeric("regulizar4", 0.1),
  flag_integer("epochs1", 32),
  flag_numeric("batchsize1", 32),
  flag_numeric("learning1", 0.01)
)

#run model
model_exhaust <- keras_model_sequential () %>%
layer_dense(units = FLAGS$units1, activation = "relu", kernel_regularizer = regularizer_l1(FLAGS$regulizar1)) %>%
layer_dropout(rate=FLAGS$dropout1) %>%
layer_dense(units = FLAGS$units2, activation = "relu", kernel_regularizer = regularizer_l1(FLAGS$regulizar2)) %>%
layer_dropout(rate=FLAGS$dropout2) %>%
layer_dense(units = FLAGS$units3, activation = "relu", kernel_regularizer = regularizer_l1(FLAGS$regulizar3)) %>%
layer_dropout(rate=FLAGS$dropout3) %>%
layer_dense(units = FLAGS$units4, activation = "relu", kernel_regularizer = regularizer_l1(FLAGS$regulizar4)) %>%
layer_dropout(rate=FLAGS$dropout4) %>%
  layer_dense(units = 1)
model_exhaust %>% compile(loss = "mse",
optimizer = optimizer_adam(learning_rate=FLAGS$learning1))


#use tuning run with model and data in additional R script, attempted numerous combinations to assist with computational time
runs_epochs <- tuning_run("hyperparametertuning.R", flags = list(
  dropout1 = c(.2,.3,.4), 
  dropout2 = c(.2,.3.,4), 
  dropout3 = c(.2,.3,.4),
  dropout4 = c(.2,.3,.4),
  units1 = c(32,64),
  units2 = c(32,64),
  units3 = c(32,64),
  units4=  c(32,64),
  regulizar1 = c(.00001),
  regulizar2 = c(.00001),
  regulizar3 = c(.00001),
  regulizar4 = c(.00001),
  epochs1 = c(100,200,300),
  batchsize1 = c(30,100,400, 1000, 4000),
  learning1 = c(.001)
), confirm=FALSE)

#print best combination based on mse
runs_epochs[order(runs_epochs$eval_, decreasing = FALSE), ]
####################################################################################################################################
#access information for each combination
#when running replace "runs/2022-04..." with the results got
runs$flag_dropout1[runs$run_dir == "runs/2022-04-16T02-08-47Z"]
runs$flag_dropout2[runs$run_dir == "runs/2022-04-16T02-08-47Z"]
runs$flag_dropout3[runs$run_dir == "runs/2022-04-16T02-08-47Z"]
runs$flag_dropout4[runs$run_dir == "runs/2022-04-16T02-08-47Z"]

#number one choice
runs$flag_units1[runs$run_dir == "runs/2022-04-16T03-31-16Z"]
runs$flag_units2[runs$run_dir == "runs/2022-04-16T03-31-16Z"]
runs$flag_units3[runs$run_dir == "runs/2022-04-16T03-31-16Z"]
runs$flag_units4[runs$run_dir == "runs/2022-04-16T03-31-16Z"]

#number two choice
runs$flag_units1[runs$run_dir == "runs/2022-04-16T03-16-48Z"]
runs$flag_units2[runs$run_dir == "runs/2022-04-16T03-16-48Z"]
runs$flag_units3[runs$run_dir == "runs/2022-04-16T03-16-48Z"]
runs$flag_units4[runs$run_dir == "runs/2022-04-16T03-16-48Z"]
```

```{r Classification Test and Training Data}
#CREATING THE TEST AND TRAINING DATA
#first take out NA's in data frame we will use
data_diet <-drop_na(data, DRQSDIET, BMXWT, LBXTC, BPXSY1, DR1_330Z, DR1TCAFF)
data_diet<-filter(data_diet, DRQSDIET != 9)

#first split in test and train for whole data frame
train_idx <- sample(1:nrow(data_diet), size = floor(.7 * nrow(data_diet)))

#test and training data
train_data <- data_diet[train_idx, ]
test_data <- data_diet[-train_idx, ]
test_idx <- seq(nrow(data_diet))[-train_idx]

#change to factor for test data
test_data<-filter(test_data, DRQSDIET != 9)
test_data$DRQSDIET01 <- ifelse(test_data$DRQSDIET == 2, 1, 0)

#change to factor for training data
train_data<-filter(train_data, DRQSDIET != 9)
train_data$DRQSDIET01 <- ifelse(train_data$DRQSDIET == 2, 1, 0)
```

```{r Knn for Classification}
#SCALE FOR KNN
#calculate mean and sd
mean_train <- colMeans(train_data[c('BMXWT','LBXTC', 'BPXSY1', 'DR1_330Z', 'DR1TCAFF')])
sd_train <- sqrt(diag(var(train_data[c('BMXWT','LBXTC', 'BPXSY1', 'DR1_330Z', 'DR1TCAFF')])))
#scale the training and test data
X_train_scaled <- scale(train_data[c('BMXWT','LBXTC', 'BPXSY1', 'DR1_330Z', 'DR1TCAFF')], center = mean_train, scale = sd_train)
X_test_scaled <- scale(test_data[c('BMXWT','LBXTC', 'BPXSY1', 'DR1_330Z', 'DR1TCAFF')], center = mean_train, scale = sd_train)


#run cross validation in order to find ideal K on training set
Ks <- seq(1, 352, by = 10)

#vectors to store the MSEs
trainMSEs <- c()
testMSEs <- c() 

for(K in Ks) {
knnTrain <- knn(train = X_train_scaled, cl = train_data$DRQSDIET,
test = X_train_scaled, k = K)
knnTest <- knn(train = X_train_scaled, cl = train_data$DRQSDIET,
test = X_test_scaled, k = K)

trainMSE <- mean(knnTrain != train_data$DRQSDIET)
testMSE <- mean(knnTest != test_data$DRQSDIET)

trainMSEs <- c(trainMSEs, trainMSE)
testMSEs <- c(testMSEs, testMSE)
}
#plot the train and test errors against the K value
plot(Ks, trainMSEs, type = "b", lwd = 2, col = "blue",
xlab = "K", ylab = "MSE")
lines(Ks, testMSEs, type = "b", lwd = 2, col = "red")
legend("bottomright", legend = c("Train MSE", "Test MSE"), col = c("blue", "red"), lwd = c(2,2))

#MUCH SMALLER SEQUENCE
Ks <- seq(1, 40, by = 1)

#vectors to store the MSEs
trainMSEs <- c()
testMSEs <- c() 

for(K in Ks) {
knnTrain <- knn(train = X_train_scaled, cl = train_data$DRQSDIET,
test = X_train_scaled, k = K)
knnTest <- knn(train = X_train_scaled, cl = train_data$DRQSDIET,
test = X_test_scaled, k = K)

trainMSE <- mean(knnTrain != train_data$DRQSDIET)
testMSE <- mean(knnTest != test_data$DRQSDIET)

trainMSEs <- c(trainMSEs, trainMSE)
testMSEs <- c(testMSEs, testMSE)
}
#plot the train and test errors against the K value
plot(Ks, trainMSEs, type = "b", lwd = 2, col = "blue", xlab = "K", ylab = "MSE", ylim = c(0,.4))
lines(Ks, testMSEs, type = "b", lwd = 2, col = "red")
legend("bottomright", legend = c("Train MSE", "Test MSE"), col = c("blue", "red"), lwd = c(2,2))

#which K produces the minimum for testMSEs
x = Ks[which.min(testMSEs)]

#KNN with this ideal K
KNN_11<-knn(train = X_train_scaled, cl = train_data$DRQSDIET,
test = X_test_scaled, k = x)
#test error
mean(KNN_11 != test_data$DRQSDIET)
#confusion matrix
table(KNN_11, test_data$DRQSDIET)
```


```{r Decision Trees/Adaboost for Classification Problem}

#Adaboost
#make adaboost model (boosted tree)
adaboost_mod <- gbm(
DRQSDIET01~ BMIWT + LBXTC + BPXSY1 + DR1_330Z + DR1TCAFF, 
data=train_data,
distribution="adaboost",
n.trees=1000)

#variable importance plot
summary(adaboost_mod)

#find test error
adaboost_test_probs <- predict(adaboost_mod, test_data, n.trees=1000, type='response')
adaboost_test_preds <- ifelse(adaboost_test_probs > 0.5, 1, 0)
test_err <- mean(adaboost_test_preds != test_data$DRQSDIET01)
print(test_err)
```

```{r SVM Classifier}

#data cleaning only for svm classifier
train_data$DRQSDIET01 <- as.factor(train_data$DRQSDIET01)
test_data$DRQSDIET01 <- as.factor(test_data$DRQSDIET01)

data_diet$DRQSDIET01 <- ifelse(data_diet$DRQSDIET == 2, 1, 0) 
data_diet$DRQSDIET01 <- as.factor(data_diet$DRQSDIET01)

#subset for only columns needed
train_data <- train_data[, c("DRQSDIET01", "BMXWT", "LBXTC", "BPXSY1", "DR1_330Z", "DR1TCAFF")]
test_data <- test_data[, c("DRQSDIET01", "BMXWT", "LBXTC", "BPXSY1", "DR1_330Z", "DR1TCAFF")]
data_diet <- data_diet[, c("DRQSDIET01", "BMXWT", "LBXTC", "BPXSY1", "DR1_330Z", "DR1TCAFF")]

#for cross validation
test_idx2 <- sample(test_idx, size=round(length(test_idx)/2))
validation_idx <- as.integer(setdiff(test_idx, test_idx2))

#function to evaluate cost for cross validation
test_cost <- function(p, test_data, vector) {
 svmfit <- svm(DRQSDIET01 ~ BMXWT + LBXTC + BPXSY1 + DR1_330Z + DR1TCAFF,
 data=train_data, # Training data set
 kernel="radial", # Kernel type
 cost=p, # Cost of misclassification
 scale=TRUE)
 preds <- predict(svmfit, na.omit(test_data))
 mean(preds != na.omit(vector))}

#run cross validation to find ideal cost value
validation_errors <- sapply(c(0.001, 0.01, 0.1, 1, 5, 10, 100),
                            FUN=function(p) test_cost(p, data_diet[validation_idx,], 
                            data_diet$DRQSDIET01[validation_idx]))
test_errors <- sapply(c(0.001, 0.01, 0.1, 1, 5, 10, 100),
                      FUN=function(p) test_cost(p, data_diet[test_idx2,], 
                      data_diet$DRQSDIET01[test_idx2]))
all_errors <- c(validation_errors, test_errors)

#plot different cost results
plot(
c(0.001, 0.01, 0.1, 1, 5, 10, 100),
validation_errors,
type="l",
xlab="Cost Values",
ylab="rMSE",
main="Cost Values vs. Test rMSE",
ylim=c(min(all_errors), max(all_errors)))
lines(c(0.001, 0.01, 0.1, 1, 5, 10, 100),
test_errors, col=2)
abline(v=3, lty=3)
legend("topright", legend=c("Validation", "Test"), col=1:2, lty=1)

#ideal cost value is 10 so run svm with cost = 10
svmfit <- svm(DRQSDIET01 ~ BMXWT + LBXTC + BPXSY1 + DR1_330Z + DR1TCAFF,
data=train_data,
kernel="radial",
cost=10, 
scale=TRUE)

#test error
preds <- predict(svmfit, test_data)
mean(preds != test_data$DRQSDIET01)

#can see differences between fitted values and actual
table(svmfit$fitted)
table(test_data$DRQSDIET01)
```