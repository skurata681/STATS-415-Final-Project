---
title: "Kaggle"
author: "Sarah Kurata"
date: "4/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(245)
#neural network
library(keras)
library(neuralnet)
library(superml)
library(tfruns)
library(dplyr)
library(readr)
library(gbm)
library(SignifReg)
library(randomForest)
```

```{r}
train = read_csv("Data/Total/train.csv")
test = read_csv("Data/Total/test.csv")

#original attempt
view(train)
model1 = glm(y ~ BMDSTATS, data = train)
x = predict(model1, newdata = test)
firstprediction = data.frame(test$SEQN, x)
firstprediction <-  firstprediction%>%rename(SEQN=test.SEQN)
firstprediction<-  firstprediction%>%rename(y=x)
write_csv(firstprediction, "Predictions/Kaggle/firstPrediction.csv")

#resulting model from p-value
forward_selection_model_signifreg <-lm(formula = y ~ SMAQUEX2 + BMXARML + DMDBORN4 + BPXSY1 + BPACSZ + DR1TSUGR + DR1TFIBE + RIDAGEYR + DR1TKCAL + BMXWT + BMXBMI + BMXHT + BMXWAIST + RIDEXMON + BMXARMC + SIAPROXY + DR1TATOC + FIALANG + DR1TS100 + SEQN + LBXTC + SIAINTRP + WTINT2YR, data = train)
summary(forward_selection_model_signifreg)

#Random Forest Model  
xx = randomForest(y ~., data = train, mtry = 48, importance = TRUE)
mean((predict(xx, test)- test$y)^2)

#Boost Tree
idealdepth = 0
idealntrees = 0
temp = .Machine$double.xmax
#Loop through various types of trees and pick model with lowest cross validation errors
for(ntrees in c(128,256,512,1024,2048,4096,8192,16384)){
  for(depth in c(2,4,8,16,32)){
    boosty = gbm(y~.-SEQN, data = train, n.trees = ntrees, interaction.depth = depth, shrinkage = .01, cv.folds = 5)
    tempy = mean(boosty$cv.error)
    if(tempy < temp){
      temp = tempy
      idealdepth = depth
      idealntrees = ntrees
    }
  }
}
#Ideal tree and calculate various metrics
boosty = gbm(y~.-SEQN, data = train, n.trees = idealntrees, interaction.depth = idealdepth, shrinkage = .01, cv.folds = 5)
testprediction <- predict(boosty, newdata=test, n.trees=idealntrees)
rsquare = (cor(predict(boosty, newdata=train, n.trees=idealntrees), train$y))^2

#scale data for neural networks
scale_train<-scale(train)
scale_train<-as.data.frame(scale_train)
test$y <- seq(1:3823)
scale_test<-scale(test)
scale_test<-as.data.frame(scale_test)

#training and testing data for neural networks
test_idx <- sample(seq(nrow(scale_train)), size= round(nrow(scale_train)*.3)) 
train_idx <- seq(nrow(scale_train))[-test_idx]
train_neural_networks <- scale_train[train_idx,]
test_neural_networks <- scale_train[test_idx,]

#convert data for neural networks, use to find best model
xTr = model.matrix(lm(y ~ . - 1, data=train_neural_networks))
yTr = train_neural_networks$y
xTe = model.matrix(lm(y ~ . - 1, data=test_neural_networks))
yTe = test_neural_networks$y

#use on found model
xTrain = model.matrix(lm(y ~ . - 1, data=scale_train))
yTrain = scale_train$y
xTest = model.matrix(lm(y ~ . - 1, data=scale_test))

##########Neural Network Model##################################################################################
modnn_2 <- keras_model_sequential () %>%
layer_dense(units = 64, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
layer_dropout(rate=0.4) %>%
layer_dense(units = 64, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
layer_dense(units = 62, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
layer_dropout(rate=0.4) %>%
layer_dense(units = 64, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
layer_dropout(rate=0.4) %>%
  layer_dense(units = 1)
modnn_2 %>% compile(loss = "mse",
optimizer = optimizer_adam(learning_rate=0.005))
start_time <- Sys.time()
history <- modnn_2 %>% fit(
xTr, yTr, epochs = 300, batch_size = 4500
)
end_time <- Sys.time()
end_time - start_time

#predictions
npred_train <- c(predict(modnn_2, xTr))
npred_test <- c(predict(modnn_2, xTe))

#test and training error
train_error<- mean((npred_train- yTr)^2)
test_error<-mean((npred_test- yTe)^2)

#write to neural.network. csv
neural_network_prediction = data.frame(test$SEQN, npred)
colnames(neural_network_prediction)
neural_network_prediction <-  neural_network_prediction%>%rename(SEQN=test.SEQN)
neural_network_prediction<-  neural_network_prediction%>%rename(y=npred)
write_csv(neural_network_prediction, "Predictions/Kaggle/neural_network_Prediction.csv")

########Hyperparamater Tuning for Neural Networks#############################################################################
FLAGS <- flags(
  flag_numeric("dropout1", 0.4),
  flag_numeric("dropout2", 0.3),
  flag_numeric("dropout3", 0.4),
  flag_numeric("dropout4", 0.3),
  flag_numeric("units1", 20),
  flag_numeric("units2", 20),
  flag_numeric("units3", 20),
  flag_numeric("units4", 20),
  flag_numeric("regulizar1", 0.1),
  flag_numeric("regulizar2", 0.1),
  flag_numeric("regulizar3", 0.1),
  flag_numeric("regulizar4", 0.1),
  flag_integer("epochs1", 32),
  flag_numeric("batchsize1", 32),
  flag_numeric("learning1", 0.01)
)

model_exhaust <- keras_model_sequential () %>%
layer_dense(units = FLAGS$units1, activation = "relu", kernel_regularizer = regularizer_l1(FLAGS$regulizar1)) %>%
layer_dropout(rate=FLAGS$dropout1) %>%
layer_dense(units = FLAGS$units2, activation = "relu", kernel_regularizer = regularizer_l1(FLAGS$regulizar2)) %>%
layer_dropout(rate=FLAGS$dropout2) %>%
layer_dense(units = FLAGS$units3, activation = "relu", kernel_regularizer = regularizer_l1(FLAGS$regulizar3)) %>%
layer_dropout(rate=FLAGS$dropout3) %>%
layer_dense(units = FLAGS$units4, activation = "relu", kernel_regularizer = regularizer_l1(FLAGS$regulizar4)) %>%
layer_dropout(rate=FLAGS$dropout4) %>%
  layer_dense(units = 1)

model_exhaust %>% compile(loss = "mse",
optimizer = optimizer_adam(learning_rate=FLAGS$learning1))

runs_epochs <- tuning_run("hyperparametertuning.R", flags = list(
  dropout1 = c(0.2,0.3,0.4), #ran
  dropout2 = c(0.2,0.3,0.4), #ran
  dropout3 = c(0.2,0.3,0.4), #ran
  dropout4 = c(0.2,0.3,0.4), #ran
  units1 = c(32,64),
  units2 = c(32,64),
  units3 = c(32,64),
  units4=  c(32,64),
  regulizar1 = c(.00001),
  regulizar2 = c(.00001),
  regulizar3 = c(.00001),
  regulizar4 = c(.00001),
  epochs1 = c(100,200,300),
  batchsize1 = c(50,400,1000,4500),
  learning1 = c(.001)
), confirm=FALSE)
runs_epochs[order(runs_epochs$eval_, decreasing = FALSE), ]

############################################################################################
#how to access best values for neural network
#use file that device gave instead of "runs/2022-04-..."
runs$flag_dropout1[runs$run_dir == "runs/2022-04-16T02-08-47Z"]
runs$flag_dropout2[runs$run_dir == "runs/2022-04-16T02-08-47Z"]
runs$flag_dropout3[runs$run_dir == "runs/2022-04-16T02-08-47Z"]
runs$flag_dropout4[runs$run_dir == "runs/2022-04-16T02-08-47Z"]

#number one choice
runs$flag_units1[runs$run_dir == "runs/2022-04-16T03-31-16Z"]
runs$flag_units2[runs$run_dir == "runs/2022-04-16T03-31-16Z"]
runs$flag_units3[runs$run_dir == "runs/2022-04-16T03-31-16Z"]
runs$flag_units4[runs$run_dir == "runs/2022-04-16T03-31-16Z"]

#number two choice
runs$flag_units1[runs$run_dir == "runs/2022-04-16T03-16-48Z"]
runs$flag_units2[runs$run_dir == "runs/2022-04-16T03-16-48Z"]
runs$flag_units3[runs$run_dir == "runs/2022-04-16T03-16-48Z"]
runs$flag_units4[runs$run_dir == "runs/2022-04-16T03-16-48Z"]
```
```

