---
title: "Regression Analysis"
author: "Saman Verma"
date: "3/27/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(readr)
library(tidyverse)
library(caret)
library(ggplot2)
library(scales)
library(glmnet)
library(Hmisc)
train = read_csv("Data/Total/train.csv")
test = read_csv("Data/Total/test.csv")
```


```{r}
#Subsetting Dataframe for demographic data and coffee
x = data.frame(train)
#Riagender = Gender, Ridageyr = Age RidReth3 = Race INDHHIN2 = Household income DMHREDU = Household Education DMDCITZN = Citizenship Status DR1TCAFF = Coffee
x = x[, which(names(x)%in%c("RIAGENDR","RIDAGEYR","RIDRETH3","INDHHIN2","DMDHREDU","DMDCITZN", "DR1TCAFF"))]
test = test[, which(names(test)%in%c("RIAGENDR","RIDAGEYR","RIDRETH3","INDHHIN2","DMDHREDU","DMDCITZN", "DR1TCAFF"))]
```


```{r}
#Check how many unique values for each categorical variables as well as means and standard deviations for others
categorical = x[, -which(names(x) %in%c("RIDAGEYR", "DR1TCAFF"))]

for (i in 1:ncol(categorical)){
  print(names(categorical)[i])
  print(table(categorical[,i])/length(categorical[,i]))
}

#The only categorical variable that seems to be completely uneven is the variable related to citizenship status which makes sense as we are studying people in America therefore the vast majority should be american citizens. Limited concern for these variables.

numerical = x[, which(names(x) %in%c("RIDAGEYR", "DR1TCAFF"))]
for (i in 1:ncol(numerical)){
  hist(numerical[,i], , xlab=names(numerical)[i], ylab = "Number of Values in this range", main = "Distribution of Numeric Column in Dataset")
}

#We see a strong right skew in the response variable. Let's log transform this column to see if it helps with the skew.
Caffeine = log(numerical[, "DR1TCAFF"])
hist(Caffeine, , xlab=names(numerical)[i], ylab = "Number of Values in this range", main = "Distribution of Log Transformed Column in Dataset")

#This looks better than the other data when it comes to normality. For models assuming normality we can simply log transform it to fit that assumption.

```


```{r}
#Remember to log transform y variable

linearregressionmodel = lm(log(DR1TCAFF)~as.factor(RIAGENDR)+RIDAGEYR+as.factor(RIDRETH3)+as.factor(INDHHIN2)+as.factor(DMDHREDU)+as.factor(DMDCITZN), data = x)
print(summary(linearregressionmodel))
prediction = predict(linearregressionmodel, newdata = test)
cat("\nOverall Error of this model on the testing data is ", mean((prediction - Ytest)**2), ".\n\n")
```

```{r}
#We shall use a lasso regression for this model. This is because lasso indirectly performs feature selection for our data as it forces some coefficients to 0 depending on its insignificance. Also, we would rather perform lasso here than ridge as we have a relatively small number of predictors as compared to the amount of data we have. It also regularizes the data so that it can perform better generally. We log transform the response variables for the assumption of normality as mentioned in exploratory data section.
logtransform = data.frame(x)
logtransform$DR1TCAFF = log(logtransform$DR1TCAFF)
logtransform = logtransform[!is.infinite(rowSums(logtransform)),]
logtransformtest = data.frame(test)
logtransformtest$DR1TCAFF = log(logtransformtest$DR1TCAFF)
logtransformtest = logtransformtest[!is.infinite(rowSums(logtransformtest)),]
Xtrain = model.matrix(DR1TCAFF~as.factor(RIAGENDR)+RIDAGEYR+as.factor(RIDRETH3)+as.factor(INDHHIN2)+as.factor(DMDHREDU)+as.factor(DMDCITZN), logtransform)[,-1]
Ytrain = logtransform$DR1TCAFF
Xtest = model.matrix(log(DR1TCAFF)~as.factor(RIAGENDR)+RIDAGEYR+as.factor(RIDRETH3)+as.factor(INDHHIN2)+as.factor(DMDHREDU)+as.factor(DMDCITZN), logtransformtest)[,-1]
Ytest = log(logtransformtest$DR1TCAFF)
Xtest = cbind(Xtest, 0)
colnames(Xtest)[30] = "as.factor(INDHHIN2)77"
Xtest = cbind(Xtest, 0)
colnames(Xtest)[31] = "as.factor(DMDHREDU)7"
bestlambda = cv.glmnet(Xtrain, Ytrain, alpha = 1)$lambda.min
modelylassoy = glmnet(Xtrain,Ytrain, alpha = 1, lambda = bestlambda)
coef(modelylassoy)
prediction = predict(modelylassoy, s = bestlambda, newx=Xtest)
cat("\nOverall Error of this model on the testing data is ", mean((prediction - Ytest)**2), ".\n\n")
```